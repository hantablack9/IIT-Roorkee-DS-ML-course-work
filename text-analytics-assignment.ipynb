{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hanpat99/text-analytics-dtm-tf-idf-vect-countvectorizer?scriptVersionId=135839810\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"- **This is a take home assignment on NLP and Text Analytics**\n\n ","metadata":{}},{"cell_type":"markdown","source":"# Topics covered:\n- Text preprocessing\n- n-grams, Visualization\n- DTM conversion of text data\n- WordCloud Visualizations ","metadata":{}},{"cell_type":"markdown","source":"Problem statement:\n\n\n\nFor the given data from the amazon reviews, reviewText column, perform the below activities:\n\n \n\n1. **Clean** the data\n\n2. **Plot** a bigram bar graph on the top words 15 words\n\n3. **Find** customer concern areas - the top 15 bigrams which includes the below negative words indicating the customer concern areas\n\n> 'poor', 'waste', 'bad', 'defective', 'disgusting', 'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow'\n\n4. **Plot** a bar graph for the top 15 customer concern areas\n\n5. Prepare **word cloud** for the above mentioned conditions","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n# Workflow\n- [0. Imports and data loading](#0.1)\n- [1. Understanding the data](#1) \n- [2. Preprocessing](#2)\n    >- [2.1. Missing Values, Duplicates Handling](#2.1)\n    >- [2.2. Convert doucuments to lower case](#2.2)\n    >- [2.3. Remove Punctuations](#2.3)\n    >- [2.4. Identify Domain words from wordcloud](#2.4)\n    >- [2.5. Import english stopwords list, remove the wordlist below, add domain words](#2.5)\n>         -> 'poor', 'waste', 'bad', 'defective', 'disgusting', 'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow'\n    >- [2.6. Build custom cleanup function and finish preprocessing](#2.6)\n- [3. Bigram visualizations: Identifying customer pain points with select keywords](#3)\n- [4. Wordcloud: Visualizing top 15 customer bigrams from the list provided](#4)\n- [5. Analysis and Recommendations](#5)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0.1\"></a>\n\n# [0. Imports and data loading](#0)","metadata":{}},{"cell_type":"code","source":"## Import block \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=15,8\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.float_format', lambda x: '%.2f' % x)\n\n#nltk\nimport nltk\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-05T17:31:00.371942Z","iopub.execute_input":"2023-07-05T17:31:00.372915Z","iopub.status.idle":"2023-07-05T17:31:03.360113Z","shell.execute_reply.started":"2023-07-05T17:31:00.372847Z","shell.execute_reply":"2023-07-05T17:31:03.358096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load dataset\npath = '/kaggle/input/amazon-reviews-dataset/Amazon_reviews_csv.xlsm'\ndf = pd.read_excel(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:03.362803Z","iopub.execute_input":"2023-07-05T17:31:03.363378Z","iopub.status.idle":"2023-07-05T17:31:04.494739Z","shell.execute_reply.started":"2023-07-05T17:31:03.363335Z","shell.execute_reply":"2023-07-05T17:31:04.493073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# [1. Understanding the data](#0)","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.497768Z","iopub.execute_input":"2023-07-05T17:31:04.498863Z","iopub.status.idle":"2023-07-05T17:31:04.549161Z","shell.execute_reply.started":"2023-07-05T17:31:04.498794Z","shell.execute_reply":"2023-07-05T17:31:04.54764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ASIN stands for Amazon Standard Identification Number.**\nEach product listed on Amazon has a unique ASIN, which helps identify and track the product. ASINs are used by Amazon to manage its product catalog and facilitate product identification, search, and categorization on the platform. ASINs are specific to Amazon and are not used as universal product identifiers.","metadata":{}},{"cell_type":"markdown","source":"**Helpful feature** typically us how much user have voted on the helpfulness of a review. A helpfulness tuple (4,6) indicates that 6 users have voted on a review, and 4 have found it to be helpful. ","metadata":{}},{"cell_type":"markdown","source":"**Overall** features describes the overall rating provided by the a reviewer and indicates the customer's overall satisfaction or opinion about the product on a numerical scale. The \"overall\" feature typically ranges from 1 to 5, with 1 being the lowest rating and 5 being the highest rating.","metadata":{}},{"cell_type":"markdown","source":"<a id ='2'></a>\n# [2. Preprocessing](#0)","metadata":{}},{"cell_type":"markdown","source":"**reviewTime feature** gives us the datetime information about the review.","metadata":{}},{"cell_type":"code","source":"## Lets convert the text value into manipulatable datetime format\ndf['reviewTime'] = pd.to_datetime(df['reviewTime'], format='%m %d, %Y')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.552448Z","iopub.execute_input":"2023-07-05T17:31:04.553001Z","iopub.status.idle":"2023-07-05T17:31:04.570036Z","shell.execute_reply.started":"2023-07-05T17:31:04.552936Z","shell.execute_reply":"2023-07-05T17:31:04.568449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## First Lets conver Unix timestamp to datetime\ndf['unixReviewTime'] = pd.to_datetime(df['unixReviewTime'], unit='s')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.572012Z","iopub.execute_input":"2023-07-05T17:31:04.573119Z","iopub.status.idle":"2023-07-05T17:31:04.594883Z","shell.execute_reply.started":"2023-07-05T17:31:04.573063Z","shell.execute_reply":"2023-07-05T17:31:04.593135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id ='2.1'></a>\n## [2.1 Missing Values, Duplicates Handling](#0)","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.597088Z","iopub.execute_input":"2023-07-05T17:31:04.597819Z","iopub.status.idle":"2023-07-05T17:31:04.628706Z","shell.execute_reply.started":"2023-07-05T17:31:04.597766Z","shell.execute_reply":"2023-07-05T17:31:04.627449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I will drop where reviewText is NA, since they do not provide any information for my analysis.**","metadata":{}},{"cell_type":"code","source":"df = df.dropna(subset=['reviewText'])\ndf['reviewerName'] = df['reviewerName'].fillna('Missing')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.630824Z","iopub.execute_input":"2023-07-05T17:31:04.631739Z","iopub.status.idle":"2023-07-05T17:31:04.671502Z","shell.execute_reply.started":"2023-07-05T17:31:04.631684Z","shell.execute_reply":"2023-07-05T17:31:04.670266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_text = df['reviewText']\nreview_text","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.673609Z","iopub.execute_input":"2023-07-05T17:31:04.674499Z","iopub.status.idle":"2023-07-05T17:31:04.687923Z","shell.execute_reply.started":"2023-07-05T17:31:04.674432Z","shell.execute_reply":"2023-07-05T17:31:04.685971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id ='2.2'></a>\n## [2.2 Convert documents to lower case](#0)","metadata":{}},{"cell_type":"code","source":"## Convert to lower case\ndf['tokenized'] = df['reviewText'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.689973Z","iopub.execute_input":"2023-07-05T17:31:04.691364Z","iopub.status.idle":"2023-07-05T17:31:04.715127Z","shell.execute_reply.started":"2023-07-05T17:31:04.691301Z","shell.execute_reply":"2023-07-05T17:31:04.713578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## [2.3 Remove Punctuations](#0)","metadata":{}},{"cell_type":"code","source":"import re\n\ndef punct_remove(x):\n    reviews_combined_clean = re.sub(\"[^\\w\\s]+\",\" \",x)\n    return reviews_combined_clean","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.724371Z","iopub.execute_input":"2023-07-05T17:31:04.725234Z","iopub.status.idle":"2023-07-05T17:31:04.741355Z","shell.execute_reply.started":"2023-07-05T17:31:04.725163Z","shell.execute_reply":"2023-07-05T17:31:04.739798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tokenized'] = df['tokenized'].apply(lambda x: punct_remove(x))\ndf['tokenized'][:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.743789Z","iopub.execute_input":"2023-07-05T17:31:04.744778Z","iopub.status.idle":"2023-07-05T17:31:04.829622Z","shell.execute_reply.started":"2023-07-05T17:31:04.744723Z","shell.execute_reply":"2023-07-05T17:31:04.828339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n## [2.4 Identify domain words from wordcloud](#0)","metadata":{}},{"cell_type":"code","source":"## Get the full corpus of tokens\nfull_join = \" \".join(df['reviewText'].values)\nfull_join_list = full_join.split(' ')\n\n## Lets check the count of the special tokens\nbad10 = ['poor', 'waste', 'bad', 'defective', 'disgusting', \\\n         'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow']\nbad_10_dict= {}\nfor word in bad10:\n    bad_10_dict[word] = len(re.findall(pattern = word, string = full_join ))\nbad_10_dict","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.831315Z","iopub.execute_input":"2023-07-05T17:31:04.831756Z","iopub.status.idle":"2023-07-05T17:31:04.891508Z","shell.execute_reply.started":"2023-07-05T17:31:04.831718Z","shell.execute_reply":"2023-07-05T17:31:04.88964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets build a wordcloud now to identify domain words\n\nplt.style.use('ggplot')\nfrom wordcloud import WordCloud\n\nword_cloud = WordCloud(width=1600,height=900,\n                       background_color='white',\n                       max_words=150).generate(full_join)\n\n%config InlineBackend.figure_format=\"retina\"\nplt.figure(figsize=[8,8])\nplt.imshow(word_cloud)\nplt.savefig('Reviews word cloud.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:04.894144Z","iopub.execute_input":"2023-07-05T17:31:04.895262Z","iopub.status.idle":"2023-07-05T17:31:09.785318Z","shell.execute_reply.started":"2023-07-05T17:31:04.895203Z","shell.execute_reply":"2023-07-05T17:31:09.783974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that **nook, book, one, kindle, device, tablet,** are the domain words. Upon further research, it is clear that **the reviews are for the product \"Barnes & Noble Nook\".** Mentions of competitor products like Kindle, etc may refer to a large number of users comparing the Nook to the Kindle, in their reviews.\n\n**Let's add these above domain words to the final StopWords.**","metadata":{}},{"cell_type":"markdown","source":"**Looks like our terms of interest, the Bad10 aren't showing up in the word cloud. We have to be careful while setting up the DTM so as to not truncate our terms. Let's add these to our retain words set.**","metadata":{}},{"cell_type":"markdown","source":"Lets identify any other negative words from this corpus and remove them.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.5\"></a>\n**[2.5. Import english stopwords list, remove the wordlist below, add domain words](#0)**","metadata":{}},{"cell_type":"code","source":"## Get English StopWords and remove from documents\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_nltk = stopwords.words(\"english\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:09.786778Z","iopub.execute_input":"2023-07-05T17:31:09.787297Z","iopub.status.idle":"2023-07-05T17:31:09.913879Z","shell.execute_reply.started":"2023-07-05T17:31:09.787249Z","shell.execute_reply":"2023-07-05T17:31:09.912362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_nltk","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:09.91581Z","iopub.execute_input":"2023-07-05T17:31:09.916539Z","iopub.status.idle":"2023-07-05T17:31:09.930043Z","shell.execute_reply.started":"2023-07-05T17:31:09.916494Z","shell.execute_reply":"2023-07-05T17:31:09.92844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retain_set = {'against','ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n \"wouldn't\",'don', \"don't\", 'no', 'nor', 'not', 'only','from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n 'again', 'further', 'then', 'once', 'here', 'poor', 'waste', 'bad', 'defective', 'disgusting', \n         'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow' }\n\ndrop_set = {'no problem', 'problem', 'nook', 'book', 'one', 'device', \n'barnes', 'noble', 'barnes and noble', 'barnes noble', 'barnes & noble'}","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:09.932241Z","iopub.execute_input":"2023-07-05T17:31:09.932854Z","iopub.status.idle":"2023-07-05T17:31:09.94396Z","shell.execute_reply.started":"2023-07-05T17:31:09.93281Z","shell.execute_reply":"2023-07-05T17:31:09.942314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words_updated = list(set(stop_nltk).union(drop_set) - retain_set)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:09.946679Z","iopub.execute_input":"2023-07-05T17:31:09.947643Z","iopub.status.idle":"2023-07-05T17:31:09.958895Z","shell.execute_reply.started":"2023-07-05T17:31:09.94759Z","shell.execute_reply":"2023-07-05T17:31:09.957579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.6\"></a>\n## [2.6. Build custom function and finish preprocessing](#0)","metadata":{}},{"cell_type":"markdown","source":"Now that our **stop words list is finalized**, and basic preprocessing is done, let's also **lemmatize our documents** and do the final preprocessing tasks.","metadata":{}},{"cell_type":"code","source":"## Lemmatize all documents in the dataset\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nlemma = WordNetLemmatizer()\n\n## Uncomment and run only if encountering an error during document_lemmatizer() function calls\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-05T17:31:09.961632Z","iopub.execute_input":"2023-07-05T17:31:09.962864Z","iopub.status.idle":"2023-07-05T17:31:11.504079Z","shell.execute_reply.started":"2023-07-05T17:31:09.962773Z","shell.execute_reply":"2023-07-05T17:31:11.502317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n## Custom function to clean text, lemmatize, and drop stop_words\ndef document_lemmatizer(document):\n    ## Trim white spaces\n    sent = document.strip()\n    \n    ## Replace non-alphanumeric and non-space charecters with space\n    result1 = re.sub(\"[^\\w\\s]\",\" \",sent)\n    \n    ## Replace any multiple spaces with a single space\n    result = re.sub(\"\\s+\", \" \", result1)\n    tokens = word_tokenize(result.lower())\n    lemm_token = [lemma.lemmatize(term) \n                  for term in tokens \n                  if term not in stop_words_updated \n                  and len(term)>2]\n    \n    result = \" \".join(lemm_token)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:11.507353Z","iopub.execute_input":"2023-07-05T17:31:11.507975Z","iopub.status.idle":"2023-07-05T17:31:11.517984Z","shell.execute_reply.started":"2023-07-05T17:31:11.507914Z","shell.execute_reply":"2023-07-05T17:31:11.516285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tokenized'] = df['tokenized'].apply(lambda x: document_lemmatizer(x))\ndf['tokenized']","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:11.520232Z","iopub.execute_input":"2023-07-05T17:31:11.520721Z","iopub.status.idle":"2023-07-05T17:31:16.938967Z","shell.execute_reply.started":"2023-07-05T17:31:11.520682Z","shell.execute_reply":"2023-07-05T17:31:16.93752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets build a wordcloud now to identify\n\nclean_tokens_join = \" \".join(df['tokenized'].values)\nfrom wordcloud import WordCloud\n\nword_cloud = WordCloud(width=1600,height=900,\n                       background_color='white',\n                       max_words=150).generate(clean_tokens_join)\n%config InlineBackend.figure_format=\"retina\"\nplt.figure(figsize=[8,8])\nplt.imshow(word_cloud)\nplt.savefig('Cleaned Reviews word cloud.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:16.940705Z","iopub.execute_input":"2023-07-05T17:31:16.941117Z","iopub.status.idle":"2023-07-05T17:31:21.743505Z","shell.execute_reply.started":"2023-07-05T17:31:16.94108Z","shell.execute_reply":"2023-07-05T17:31:21.742021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create bigrams for documents\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n## create a bigram count vectorizer object\nbigram_count_vec = CountVectorizer(ngram_range=(2,2))\nbigram_tfidf = TfidfVectorizer(ngram_range=(2,2)) \n\n## Build bigram sparse matrices usind CountVectorizer and Tf-IDF Vectorizer\nX_bigram_tfidf = bigram_tfidf.fit_transform(df['tokenized'])\nX_bigram_count_vec=  bigram_count_vec.fit_transform(df['tokenized'])\n\n## Building Document-Term-Matrix with countvectorizer and TF-IDF\nDTM_bigram_tfidf  = pd.DataFrame(X_bigram_tfidf.toarray(), columns = bigram_tfidf.get_feature_names_out())\nDTM_bigram_count_vec = pd.DataFrame(X_bigram_count_vec.toarray(), columns = bigram_count_vec.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:21.745845Z","iopub.execute_input":"2023-07-05T17:31:21.746651Z","iopub.status.idle":"2023-07-05T17:31:23.166346Z","shell.execute_reply.started":"2023-07-05T17:31:21.746604Z","shell.execute_reply":"2023-07-05T17:31:23.164786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Top 15 bigrams using tf-idf vectorizer\nplt.style.use('ggplot')\n%config InlineBackend.figure_format=\"retina\"\n\nDTM_bigram_tfidf.sum().sort_values(ascending=False).head(15).plot.bar(figsize=(20,5))\n\nplt.xticks(rotation =65, fontsize=16)\nplt.title('Top 15 Bigrams using TF-IDF technique', fontsize = 16)\nplt.savefig('Top 15 Bigrams using TF-IDF technique.png')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:23.16837Z","iopub.execute_input":"2023-07-05T17:31:23.169756Z","iopub.status.idle":"2023-07-05T17:31:24.670654Z","shell.execute_reply.started":"2023-07-05T17:31:23.169676Z","shell.execute_reply":"2023-07-05T17:31:24.66914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Top 15 bigrams using count vectorizer\n\nplt.style.use('ggplot')\n%config InlineBackend.figure_format=\"retina\"\n\nDTM_bigram_count_vec.sum().sort_values(ascending=False).head(15).plot.bar(figsize=(20,5))\n\nplt.xticks(rotation =65, fontsize=16)\nplt.title('Top 15 Bigrams using Count Vectorizer technique', fontsize = 16)\nplt.savefig('Top 15 Bigrams using Count Vectorizer technique.png')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:24.672298Z","iopub.execute_input":"2023-07-05T17:31:24.672731Z","iopub.status.idle":"2023-07-05T17:31:25.94696Z","shell.execute_reply.started":"2023-07-05T17:31:24.672696Z","shell.execute_reply":"2023-07-05T17:31:25.945575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tasks 1 & 2 - We have completed cleaning and visualizing top 15 bigrams by frequency.**\n\nLets now pay attention to the **Bad 10** - bigrams containing the terms in the below list:\n> ['poor', 'waste', 'bad', 'defective', 'disgusting', 'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow']","metadata":{}},{"cell_type":"code","source":"DTM_bigram_tfidf.sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:25.948989Z","iopub.execute_input":"2023-07-05T17:31:25.94986Z","iopub.status.idle":"2023-07-05T17:31:26.144916Z","shell.execute_reply.started":"2023-07-05T17:31:25.949802Z","shell.execute_reply":"2023-07-05T17:31:26.143587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DTM_bigram_count_vec.sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:26.146584Z","iopub.execute_input":"2023-07-05T17:31:26.147015Z","iopub.status.idle":"2023-07-05T17:31:26.217493Z","shell.execute_reply.started":"2023-07-05T17:31:26.146981Z","shell.execute_reply":"2023-07-05T17:31:26.216276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_bigrams_list = list(DTM_bigram_tfidf.columns)\nprint('Total bigrams using Tf-IDf Vectorizer:', len(tfidf_bigrams_list))\n\ncount_vec_bigrams_list = list(DTM_bigram_count_vec.columns)\nprint('Total bigrams using Count Vectorizer:', len(count_vec_bigrams_list))","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:26.219034Z","iopub.execute_input":"2023-07-05T17:31:26.219611Z","iopub.status.idle":"2023-07-05T17:31:26.261233Z","shell.execute_reply.started":"2023-07-05T17:31:26.219568Z","shell.execute_reply":"2023-07-05T17:31:26.259671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# [3. Bigram visualizations: Identifying customer pain points with select keywords](#0)","metadata":{}},{"cell_type":"code","source":"## Specify the words to search for\nbad_10_list = ['poor', 'waste', 'bad', 'defective', 'disgusting', \\\n                'untrusty', 'worst', 'horrible', 'unexpectedly', 'slow']\n\n## List to store the features containing the target words\ntfidf_target_bad10_bigrams = []\n## Iterate through each feature name\nfor feature in DTM_bigram_tfidf.columns:\n    if any(word in feature for word in bad_10_list):\n        tfidf_target_bad10_bigrams.append(feature)\n\ncount_vec_bad10_bigrams = []        \nfor feature in DTM_bigram_count_vec.columns:\n    if any(word in feature for word in bad_10_list):\n        count_vec_bad10_bigrams.append(feature)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:26.269784Z","iopub.execute_input":"2023-07-05T17:31:26.270278Z","iopub.status.idle":"2023-07-05T17:31:26.526715Z","shell.execute_reply.started":"2023-07-05T17:31:26.270238Z","shell.execute_reply":"2023-07-05T17:31:26.525406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Customer pain points with Tf-IDf of reviews**","metadata":{}},{"cell_type":"code","source":"## Lets prepare a bar-chart on our top bad review bigrams based on Tf-IDf\nplt.figure(figsize=(4, 8))\nDTM_bigram_tfidf[tfidf_target_bad10_bigrams].sum().sort_values(ascending=False)[:15].plot.barh()\nplt.xticks(fontsize=14) \nplt.xlabel('TF-IDF',fontsize = 16)\nplt.title('Customer Painpoints: Bigrams of Reviews (Tf-IDf)')\nplt.savefig('Customer Painpoints: Bigrams of Reviews (Tf-IDf).jpg')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:26.528454Z","iopub.execute_input":"2023-07-05T17:31:26.528874Z","iopub.status.idle":"2023-07-05T17:31:27.221174Z","shell.execute_reply.started":"2023-07-05T17:31:26.528838Z","shell.execute_reply":"2023-07-05T17:31:27.219688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 15 terms isn't very informative about customer pain points. Lets check again with top 30 bigrams.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 10))\nDTM_bigram_tfidf[tfidf_target_bad10_bigrams].sum().sort_values(ascending=False)[:45].plot.barh();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:27.223082Z","iopub.execute_input":"2023-07-05T17:31:27.223603Z","iopub.status.idle":"2023-07-05T17:31:28.078338Z","shell.execute_reply.started":"2023-07-05T17:31:27.22356Z","shell.execute_reply":"2023-07-05T17:31:28.076884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Some top customer pain points based on the TF-IDF of Amazon review bigrams**\n1. Value for money\n2. Slow response\n3. Poor app navigation\n4. Poor conceptualization\n5. Quality issues\n6. Misbranded product\n7. Poor contrast\n8. Heavy\n9. Defective items delivered","metadata":{}},{"cell_type":"markdown","source":"## **Customer pain points with Count Vectorized reviews**","metadata":{}},{"cell_type":"code","source":"## Lets prepare a bar-chart on our top bad review bigrams based on Count Vectorizing\nplt.figure(figsize=(4, 8))\nDTM_bigram_count_vec[count_vec_bad10_bigrams].sum().sort_values(ascending=False)[:15].plot.barh()\nplt.xticks(fontsize=14)\nplt.xlabel('Counts',fontsize = 16)\nplt.title('Customer Painpoints: Bigrams of Reviews (Count_Vec)')\nplt.savefig('Customer Painpoints: Bigrams of Reviews (Count_Vec).jpg')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:28.079871Z","iopub.execute_input":"2023-07-05T17:31:28.080293Z","iopub.status.idle":"2023-07-05T17:31:28.769089Z","shell.execute_reply.started":"2023-07-05T17:31:28.080257Z","shell.execute_reply":"2023-07-05T17:31:28.767405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 15 terms isn't very informative about customer pain points. Lets check again with top 30 bigrams.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 10))\nDTM_bigram_count_vec[count_vec_bad10_bigrams].sum().sort_values(ascending=False)[:30].plot.barh();","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:28.771133Z","iopub.execute_input":"2023-07-05T17:31:28.771621Z","iopub.status.idle":"2023-07-05T17:31:29.451334Z","shell.execute_reply.started":"2023-07-05T17:31:28.771583Z","shell.execute_reply":"2023-07-05T17:31:29.450042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Some top customer pain points based on Count Vectorizing Amazon review bigrams**\n1. Not getting their value for money\n2. Slow response\n3. Poor contrast\n4. Bad HD quality\n5. Poor customer service\n6. Not taking responsibility for defective items\n7. Questionable item quality","metadata":{}},{"cell_type":"markdown","source":"<a id = '4'></a>\n# [4. Word Cloud: visualizing top 15 customer bigrams from the list provided](#0)","metadata":{}},{"cell_type":"markdown","source":"**Lets Now build wordclouds on our top 15 problems**","metadata":{}},{"cell_type":"code","source":"tfidf_series = DTM_bigram_tfidf[tfidf_target_bad10_bigrams].sum()*100\ntfidf_series = tfidf_series.map('{:.0f}'.format).astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:29.453196Z","iopub.execute_input":"2023-07-05T17:31:29.453635Z","iopub.status.idle":"2023-07-05T17:31:29.470194Z","shell.execute_reply.started":"2023-07-05T17:31:29.453599Z","shell.execute_reply":"2023-07-05T17:31:29.468783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"('worked bad'+' ')*2","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:29.471996Z","iopub.execute_input":"2023-07-05T17:31:29.472465Z","iopub.status.idle":"2023-07-05T17:31:29.482037Z","shell.execute_reply.started":"2023-07-05T17:31:29.472426Z","shell.execute_reply":"2023-07-05T17:31:29.480434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_join = \" \"\nfor idx, (bigram, weight) in enumerate(zip(list(tfidf_series.index), list(tfidf_series.values))):\n    tfidf_join = tfidf_join + (bigram + ' ')*weight","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:29.484205Z","iopub.execute_input":"2023-07-05T17:31:29.484759Z","iopub.status.idle":"2023-07-05T17:31:29.49608Z","shell.execute_reply.started":"2023-07-05T17:31:29.484698Z","shell.execute_reply":"2023-07-05T17:31:29.494715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_join[:1400]","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:29.498338Z","iopub.execute_input":"2023-07-05T17:31:29.499048Z","iopub.status.idle":"2023-07-05T17:31:29.517345Z","shell.execute_reply.started":"2023-07-05T17:31:29.49898Z","shell.execute_reply":"2023-07-05T17:31:29.516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets build a wordcloud now to identify\n\nword_cloud = WordCloud(width=1600,height=900,\n                       background_color='white',\n                       max_words=150).generate(tfidf_join)\n%config InlineBackend.figure_format=\"retina\"\nplt.figure(figsize=[8,8])\nplt.imshow(word_cloud)\nplt.savefig('Wordcloud with top customer issues (tfidf).png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:29.518611Z","iopub.execute_input":"2023-07-05T17:31:29.519013Z","iopub.status.idle":"2023-07-05T17:31:33.402249Z","shell.execute_reply.started":"2023-07-05T17:31:29.518979Z","shell.execute_reply":"2023-07-05T17:31:33.401256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's build another Wordcloud from Count Vecorized bigrams**","metadata":{}},{"cell_type":"code","source":"count_vec_series = DTM_bigram_count_vec[count_vec_bad10_bigrams].sum()\ncount_vec_series = count_vec_series.map('{:.0f}'.format).astype(int)\n\ncount_vec_series_join = \" \"\nfor idx, (bigram, weight) in enumerate(zip(list(count_vec_series.index), list(count_vec_series.values))):\n    count_vec_series_join = count_vec_series_join + (bigram + ' ')*weight\n\n## Lets build a wordcloud now to identify\n\nword_cloud = WordCloud(width=1600,height=900,\n                       background_color='white',\n                       max_words=150).generate(count_vec_series_join)\n%config InlineBackend.figure_format=\"retina\"\nplt.figure(figsize=[8,8])\nplt.imshow(word_cloud)\nplt.savefig('Wordcloud with top customer issues (count_vect).png')\nplt.show()   ","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:31:33.403655Z","iopub.execute_input":"2023-07-05T17:31:33.404777Z","iopub.status.idle":"2023-07-05T17:31:37.02843Z","shell.execute_reply.started":"2023-07-05T17:31:33.404734Z","shell.execute_reply":"2023-07-05T17:31:37.027411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = '5'></a>\n# [5. Analysis and Recommendations](#0)\nI have compared customer pain points using both CountVectorizing and by calculating the TF-IDF of the bigrams of reviews. Obviously, a deeper investigation is needed to analyze these issues accurately. \n\n**Based on preliminary analysis, I have identified the top 5 key areas of concern and provided my recommendations for handling these issues.**\n\n**The top customer pain points about the product Barnes & Noble Nook are:**\n- **1. Overpriced:** Not getting value for their money. Probably needs a look at their **pricing strategy and the customer segments** these reviews are coming from.\n- **2. Device responsiveness:** Some users report a slowdown after purchase, and also poor customer service. Company needs to **revamp their customer servicing or risk customer churn** to competitor brands like Amazon Kindle.\n- **3. App Navigation and display issues:** This feedback should be redirected to their **R&D deparment.**\n- **4. Design issues:** Poor conceptualization, heavy, quality issues, misbranding. This is a specific **challenge to the Marketing and R&D team.** Material quality and design shouldn't be compromised since customers are reporting it as overpriced.\n- **5. Delivering defective items:** This concerns the Supply Chain Management department. Regular QA/QC checks at the warehouses to be implemented. **Need to upgrade standards for 3rd party warehousing.** Need to ensure trusted vendors are partnered up till the last mile.","metadata":{}}]}